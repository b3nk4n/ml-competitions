{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pump it Up: Data Mining the Water Table\n",
    "\n",
    "### Can you predict which water pumps are faulty?\n",
    "Using data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
    "\n",
    "Competition:\n",
    "https://www.drivendata.org/competitions/7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train_clean.csv'\n",
    "TEST_PATH = 'data/test_clean.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH,\n",
    "                       index_col='id')\n",
    "test_df = pd.read_csv(TEST_PATH,\n",
    "                      index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df.pop('status_group').copy()\n",
    "train_data = train_df.copy()\n",
    "test_data = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (59400, 15)\n",
      "train_labels shape: (59400,)\n",
      "test_data shape: (14850, 15)\n"
     ]
    }
   ],
   "source": [
    "def print_shapes():\n",
    "    print('train_data shape: {}'.format(train_data.shape))\n",
    "    print('train_labels shape: {}'.format(train_labels.shape))\n",
    "    print('test_data shape: {}'.format(test_data.shape))\n",
    "\n",
    "print_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of numerical attributes\n",
    "Also possible to do this after 1-hot-encoding of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all numeric columns to float\n",
    "def int_columns_to_float(df: pd.DataFrame):\n",
    "    for cname in df.columns:\n",
    "        if df[cname].dtype == int:\n",
    "            df[cname] = df[cname].astype(float)\n",
    "\n",
    "int_columns_to_float(train_data)\n",
    "int_columns_to_float(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_columns(df: pd.DataFrame):\n",
    "    cols = []\n",
    "    for cname in df.columns:\n",
    "        if df[cname].dtype != object:\n",
    "            cols.append(cname)\n",
    "    return cols\n",
    "    \n",
    "numeric_cols = numeric_columns(train_data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data[numeric_cols])\n",
    "train_data[numeric_cols] = scaler.transform(train_data[numeric_cols])\n",
    "test_data[numeric_cols] = scaler.transform(test_data[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical attributes\n",
    "- LabelEncoder (ordered)\n",
    "- LabelBinarizer / get_dummies (unordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.get_dummies(train_data)\n",
    "train_data.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.get_dummies(test_data)\n",
    "test_data.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (59400, 158)\n",
      "train_labels shape: (59400,)\n",
      "test_data shape: (14850, 158)\n"
     ]
    }
   ],
   "source": [
    "print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.as_matrix()\n",
    "train_data = train_data.as_matrix()\n",
    "test_data = test_data.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "WD = 5e-6\n",
    "DROP_RATE = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# set random seed after resetting the graph\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(4422)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.create_global_step()\n",
    "\n",
    "x_ph = tf.placeholder(tf.float32, shape=[None, train_data.shape[-1]])\n",
    "y_ph = tf.placeholder(tf.int64, shape=[None])\n",
    "is_training_ph = tf.placeholder_with_default(False, shape=[])\n",
    "\n",
    "def inference(x, is_training):\n",
    "    z = tf.layers.dense(x,100,\n",
    "                        activation=tf.nn.elu,\n",
    "                        kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(WD))\n",
    "    #z = tf.layers.batch_normalization(z, training=is_training)\n",
    "    z = tf.layers.dense(z, 100,\n",
    "                        activation=tf.nn.elu,\n",
    "                        kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(WD))\n",
    "    #z = tf.layers.batch_normalization(z, training=is_training)\n",
    "    z = tf.layers.dense(z, 100,\n",
    "                        activation=tf.nn.elu,\n",
    "                        kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(WD))\n",
    "    z = tf.layers.dropout(z, rate=DROP_RATE, training=is_training)\n",
    "    logits = tf.layers.dense(z, 3,\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(WD))\n",
    "    return logits\n",
    "\n",
    "logits = inference(x_ph, is_training_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_one_hot = tf.one_hot(y_ph, depth=3)\n",
    "cost_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits))\n",
    "\n",
    "wd_costs = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "if len(wd_costs) > 0:\n",
    "    total_cost_op = cost_op + tf.add_n(wd_costs, 'wd_costs')\n",
    "else:\n",
    "    total_cost_op = cost_op\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(LR).minimize(total_cost_op, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_op = tf.argmax(logits, 1)\n",
    "correct_prediction = tf.equal(prediction_op, tf.cast(y_ph, dtype=tf.int64))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_together(data, targets):\n",
    "    perm = np.random.permutation(data.shape[0])\n",
    "    return data[perm], targets[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 200\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}...'.format(epoch + 1))\n",
    "    # shuffle data\n",
    "    X_train, y_train = shuffle_together(X_train, y_train)\n",
    "    \n",
    "    batches_per_epoch = int(X_train.shape[0] / batch_size)\n",
    "    for step in range(batches_per_epoch):\n",
    "        batch_x = X_train[step*batch_size:(step+1)*batch_size]\n",
    "        batch_y = y_train[step*batch_size:(step+1)*batch_size]\n",
    "        feed = {\n",
    "            x_ph: batch_x,\n",
    "            y_ph: batch_y,\n",
    "            is_training_ph: True\n",
    "        }\n",
    "        _, cost = sess.run([train_op, cost_op], feed_dict=feed)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('@{:<3}: {:.4f}'.format(step, cost))\n",
    "            \n",
    "    # validation\n",
    "    cost, accuracy = sess.run([cost_op, accuracy_op], feed_dict= {\n",
    "        x_ph: X_test,\n",
    "        y_ph: y_test\n",
    "    })\n",
    "    print('>>> Validation: cost: {:.4f}, accuracy: {:.3f}%'.format(cost, accuracy * 100))\n",
    "    cost, accuracy = sess.run([cost_op, accuracy_op], feed_dict= {\n",
    "        x_ph: X_train,\n",
    "        y_ph: y_train\n",
    "    })\n",
    "    print('>>> Val@train: cost: {:.4f}, accuracy: {:.3f}%'.format(cost, accuracy * 100))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = (train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n",
      "@0  : 0.4463\n",
      "@100: 0.4667\n",
      "@200: 0.4313\n",
      ">>> Val@train: cost: 0.4608, accuracy: 81.024%\n",
      "Starting epoch 2...\n",
      "@0  : 0.4914\n",
      "@100: 0.4687\n",
      "@200: 0.4972\n",
      ">>> Val@train: cost: 0.4620, accuracy: 80.891%\n",
      "Starting epoch 3...\n",
      "@0  : 0.4962\n",
      "@100: 0.3761\n",
      "@200: 0.5546\n",
      ">>> Val@train: cost: 0.4625, accuracy: 80.941%\n",
      "Starting epoch 4...\n",
      "@0  : 0.4143\n",
      "@100: 0.5706\n",
      "@200: 0.4742\n",
      ">>> Val@train: cost: 0.4591, accuracy: 81.064%\n",
      "Starting epoch 5...\n",
      "@0  : 0.4051\n",
      "@100: 0.5384\n",
      "@200: 0.4989\n",
      ">>> Val@train: cost: 0.4623, accuracy: 80.939%\n",
      "Starting epoch 6...\n",
      "@0  : 0.4222\n",
      "@100: 0.5128\n",
      "@200: 0.4770\n",
      ">>> Val@train: cost: 0.4618, accuracy: 80.904%\n",
      "Starting epoch 7...\n",
      "@0  : 0.5764\n",
      "@100: 0.4466\n",
      "@200: 0.4682\n",
      ">>> Val@train: cost: 0.4558, accuracy: 81.120%\n",
      "Starting epoch 8...\n",
      "@0  : 0.5298\n",
      "@100: 0.4095\n",
      "@200: 0.4270\n",
      ">>> Val@train: cost: 0.4554, accuracy: 81.157%\n",
      "Starting epoch 9...\n",
      "@0  : 0.4403\n",
      "@100: 0.5151\n",
      "@200: 0.4528\n",
      ">>> Val@train: cost: 0.4552, accuracy: 81.273%\n",
      "Starting epoch 10...\n",
      "@0  : 0.4866\n",
      "@100: 0.4505\n",
      "@200: 0.5078\n",
      ">>> Val@train: cost: 0.4578, accuracy: 81.125%\n",
      "Starting epoch 11...\n",
      "@0  : 0.4722\n",
      "@100: 0.4384\n",
      "@200: 0.3944\n",
      ">>> Val@train: cost: 0.4528, accuracy: 81.411%\n",
      "Starting epoch 12...\n",
      "@0  : 0.5079\n",
      "@100: 0.5251\n",
      "@200: 0.4917\n",
      ">>> Val@train: cost: 0.4528, accuracy: 81.401%\n",
      "Starting epoch 13...\n",
      "@0  : 0.4353\n",
      "@100: 0.4089\n",
      "@200: 0.5336\n",
      ">>> Val@train: cost: 0.4483, accuracy: 81.603%\n",
      "Starting epoch 14...\n",
      "@0  : 0.5454\n",
      "@100: 0.4099\n",
      "@200: 0.4212\n",
      ">>> Val@train: cost: 0.4466, accuracy: 81.847%\n",
      "Starting epoch 15...\n",
      "@0  : 0.4066\n",
      "@100: 0.6173\n",
      "@200: 0.5489\n",
      ">>> Val@train: cost: 0.4498, accuracy: 81.532%\n",
      "Starting epoch 16...\n",
      "@0  : 0.4609\n",
      "@100: 0.4187\n",
      "@200: 0.4229\n",
      ">>> Val@train: cost: 0.4428, accuracy: 81.867%\n",
      "Starting epoch 17...\n",
      "@0  : 0.4582\n",
      "@100: 0.4593\n",
      "@200: 0.5526\n",
      ">>> Val@train: cost: 0.4464, accuracy: 81.657%\n",
      "Starting epoch 18...\n",
      "@0  : 0.4575\n",
      "@100: 0.4848\n",
      "@200: 0.3789\n",
      ">>> Val@train: cost: 0.4488, accuracy: 81.279%\n",
      "Starting epoch 19...\n",
      "@0  : 0.3578\n",
      "@100: 0.4225\n",
      "@200: 0.5324\n",
      ">>> Val@train: cost: 0.4397, accuracy: 81.845%\n",
      "Starting epoch 20...\n",
      "@0  : 0.4081\n",
      "@100: 0.4459\n",
      "@200: 0.4976\n",
      ">>> Val@train: cost: 0.4424, accuracy: 81.754%\n",
      "Starting epoch 21...\n",
      "@0  : 0.4110\n",
      "@100: 0.4362\n",
      "@200: 0.4433\n",
      ">>> Val@train: cost: 0.4407, accuracy: 81.710%\n",
      "Starting epoch 22...\n",
      "@0  : 0.5088\n",
      "@100: 0.3936\n",
      "@200: 0.4606\n",
      ">>> Val@train: cost: 0.4387, accuracy: 81.774%\n",
      "Starting epoch 23...\n",
      "@0  : 0.4148\n",
      "@100: 0.4429\n",
      "@200: 0.4262\n",
      ">>> Val@train: cost: 0.4365, accuracy: 82.054%\n",
      "Starting epoch 24...\n",
      "@0  : 0.4057\n",
      "@100: 0.4754\n",
      "@200: 0.4576\n",
      ">>> Val@train: cost: 0.4362, accuracy: 82.051%\n",
      "Starting epoch 25...\n",
      "@0  : 0.4492\n",
      "@100: 0.4066\n",
      "@200: 0.4398\n",
      ">>> Val@train: cost: 0.4353, accuracy: 82.145%\n",
      "Starting epoch 26...\n",
      "@0  : 0.4737\n",
      "@100: 0.4187\n",
      "@200: 0.4268\n",
      ">>> Val@train: cost: 0.4352, accuracy: 82.067%\n",
      "Starting epoch 27...\n",
      "@0  : 0.4469\n",
      "@100: 0.4179\n",
      "@200: 0.4480\n",
      ">>> Val@train: cost: 0.4340, accuracy: 82.064%\n",
      "Starting epoch 28...\n",
      "@0  : 0.4395\n",
      "@100: 0.4893\n",
      "@200: 0.4532\n",
      ">>> Val@train: cost: 0.4333, accuracy: 82.057%\n",
      "Starting epoch 29...\n",
      "@0  : 0.3760\n",
      "@100: 0.4003\n",
      "@200: 0.4057\n",
      ">>> Val@train: cost: 0.4364, accuracy: 81.912%\n",
      "Starting epoch 30...\n",
      "@0  : 0.3440\n",
      "@100: 0.4968\n",
      "@200: 0.5787\n",
      ">>> Val@train: cost: 0.4304, accuracy: 82.163%\n",
      "Starting epoch 31...\n",
      "@0  : 0.4130\n",
      "@100: 0.4199\n",
      "@200: 0.3834\n",
      ">>> Val@train: cost: 0.4329, accuracy: 81.990%\n",
      "Starting epoch 32...\n",
      "@0  : 0.4736\n",
      "@100: 0.4308\n",
      "@200: 0.3920\n",
      ">>> Val@train: cost: 0.4306, accuracy: 82.278%\n",
      "Starting epoch 33...\n",
      "@0  : 0.4289\n",
      "@100: 0.4822\n",
      "@200: 0.4567\n",
      ">>> Val@train: cost: 0.4253, accuracy: 82.382%\n",
      "Starting epoch 34...\n",
      "@0  : 0.3763\n",
      "@100: 0.4765\n",
      "@200: 0.4276\n",
      ">>> Val@train: cost: 0.4294, accuracy: 82.399%\n",
      "Starting epoch 35...\n",
      "@0  : 0.4505\n",
      "@100: 0.3808\n",
      "@200: 0.4068\n",
      ">>> Val@train: cost: 0.4254, accuracy: 82.258%\n",
      "Starting epoch 36...\n",
      "@0  : 0.4846\n",
      "@100: 0.4139\n",
      "@200: 0.4105\n",
      ">>> Val@train: cost: 0.4264, accuracy: 82.397%\n",
      "Starting epoch 37...\n",
      "@0  : 0.3900\n",
      "@100: 0.4243\n",
      "@200: 0.5050\n",
      ">>> Val@train: cost: 0.4214, accuracy: 82.529%\n",
      "Starting epoch 38...\n",
      "@0  : 0.3943\n",
      "@100: 0.3963\n",
      "@200: 0.4017\n",
      ">>> Val@train: cost: 0.4255, accuracy: 82.279%\n",
      "Starting epoch 39...\n",
      "@0  : 0.4821\n",
      "@100: 0.4575\n",
      "@200: 0.3891\n",
      ">>> Val@train: cost: 0.4217, accuracy: 82.648%\n",
      "Starting epoch 40...\n",
      "@0  : 0.5181\n",
      "@100: 0.4570\n",
      "@200: 0.4567\n",
      ">>> Val@train: cost: 0.4283, accuracy: 82.330%\n",
      "Starting epoch 41...\n",
      "@0  : 0.4834\n",
      "@100: 0.5026\n",
      "@200: 0.4809\n",
      ">>> Val@train: cost: 0.4200, accuracy: 82.672%\n",
      "Starting epoch 42...\n",
      "@0  : 0.4380\n",
      "@100: 0.4444\n",
      "@200: 0.4401\n",
      ">>> Val@train: cost: 0.4204, accuracy: 82.402%\n",
      "Starting epoch 43...\n",
      "@0  : 0.3473\n",
      "@100: 0.4470\n",
      "@200: 0.4762\n",
      ">>> Val@train: cost: 0.4181, accuracy: 82.685%\n",
      "Starting epoch 44...\n",
      "@0  : 0.4777\n",
      "@100: 0.3858\n",
      "@200: 0.4799\n",
      ">>> Val@train: cost: 0.4202, accuracy: 82.525%\n",
      "Starting epoch 45...\n",
      "@0  : 0.4223\n",
      "@100: 0.4300\n",
      "@200: 0.4543\n",
      ">>> Val@train: cost: 0.4186, accuracy: 82.749%\n",
      "Starting epoch 46...\n",
      "@0  : 0.4504\n",
      "@100: 0.4754\n",
      "@200: 0.4037\n",
      ">>> Val@train: cost: 0.4183, accuracy: 82.609%\n",
      "Starting epoch 47...\n",
      "@0  : 0.4314\n",
      "@100: 0.3689\n",
      "@200: 0.3988\n",
      ">>> Val@train: cost: 0.4170, accuracy: 82.636%\n",
      "Starting epoch 48...\n",
      "@0  : 0.3980\n",
      "@100: 0.4830\n",
      "@200: 0.4360\n",
      ">>> Val@train: cost: 0.4146, accuracy: 82.805%\n",
      "Starting epoch 49...\n",
      "@0  : 0.4017\n",
      "@100: 0.4249\n",
      "@200: 0.4306\n",
      ">>> Val@train: cost: 0.4128, accuracy: 82.874%\n",
      "Starting epoch 50...\n",
      "@0  : 0.4212\n",
      "@100: 0.3788\n",
      "@200: 0.4416\n",
      ">>> Val@train: cost: 0.4143, accuracy: 82.731%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 200\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}...'.format(epoch + 1))\n",
    "    # shuffle data\n",
    "    X_train, y_train = shuffle_together(X_train, y_train)\n",
    "    \n",
    "    batches_per_epoch = int(X_train.shape[0] / batch_size)\n",
    "    for step in range(batches_per_epoch):\n",
    "        batch_x = X_train[step*batch_size:(step+1)*batch_size]\n",
    "        batch_y = y_train[step*batch_size:(step+1)*batch_size]\n",
    "        feed = {\n",
    "            x_ph: batch_x,\n",
    "            y_ph: batch_y,\n",
    "            is_training_ph: True\n",
    "        }\n",
    "        _, cost = sess.run([train_op, cost_op], feed_dict=feed)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('@{:<3}: {:.4f}'.format(step, cost))\n",
    "            \n",
    "    # train scores\n",
    "    cost, accuracy = sess.run([cost_op, accuracy_op], feed_dict= {\n",
    "        x_ph: X_train,\n",
    "        y_ph: y_train\n",
    "    })\n",
    "    print('>>> Val@train: cost: {:.4f}, accuracy: {:.3f}%'.format(cost, accuracy * 100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4792.,    0.,    0.,    0.,    0.,  425.,    0.,    0.,    0.,\n",
       "        9633.]),\n",
       " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a0c2ec5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = sess.run(prediction_op, feed_dict= {\n",
    "        x_ph: test_data,\n",
    "    })\n",
    "plt.hist(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(predictions, test):\n",
    "    data = {'id': test.index, 'status_group': predictions}\n",
    "\n",
    "    submit = pd.DataFrame(data=data)\n",
    "\n",
    "    vals_to_replace = {0:'non functional',\n",
    "                       1:'functional needs repair',\n",
    "                       2:'functional'}\n",
    "\n",
    "    submit['status_group'] = submit['status_group'].replace(vals_to_replace)        \n",
    "\n",
    "    submit.to_csv('pump_predictions.csv', index=False)\n",
    "    \n",
    "save_submission(prediction, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
